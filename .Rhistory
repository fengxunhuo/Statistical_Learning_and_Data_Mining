library("MASS")
library("class")
library("tidyverse")
#### Search for the help documents about this dataset ####
?biopsy
#### Get data ####
data(biopsy)
biopsy[1:5, ]
#### Delete the missing value ####
biopsy2 <- biopsy[!is.na(biopsy$V6), ]
#### Set labels, benign for 0 and malignant for 1 #####
len_bio <- nrow(biopsy2) ## Row number of the dataset
label <- rep(1, len_bio) ## Initial label
biopsy2 <- cbind(biopsy2, label) ## Add labels to biopsy2
biopsy2[which(biopsy2$class == "benign"), ]$label <- 0  ## Set label 0 for benign
############################################################################################
############# Randomly split the data into a training set and a test set####################
############# containing about 2/3 and 1/3 of total observations respectively.##############
############################################################################################
#### Split data into a training set and a test set ####
set.seed(1234) ## Initial seeds
len_train <- round(len_bio / 3 * 2)
len_test <- round(len_bio / 3)
train_num <- sample(len_bio, len_train) ## Get the 2/3 dataset's indexes randomly.
biopsy_train <- biopsy2[train_num, 2:12] ## Get training set
y_train <- biopsy2[train_num, 12]
biopsy_test <- biopsy2[-train_num, 2:12] ## Get test set
y_test <- biopsy2[-train_num, 12]
##################################### Linear Regression ######################################
fit_LR <- lm(label ~V1 + V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9, data = biopsy_train)
summary(fit_LR)
#### Training Error ####
ypred <- predict(fit_LR)  ## Use coeffient to calculate y -- label
lmtrain <-
cbind(biopsy_train, ypred) %>%
select(label, ypred) %>%
mutate(diff = abs(ypred - label))  ## Calculate differences between predicted value and actural value
lmerrTr <- sum(lmtrain$diff > 0.5) / len_train ## Calculate training data error ratio
#### Testing Error ####
test_lmnew <- biopsy_test[, 1:9] ## Get predictors in dataset
ytpred <- predict(fit_LR, newdata = test_lmnew) ## Use coeffient to calculate y -- label
lmtest <-
cbind(biopsy_test, ytpred) %>%
select(label, ytpred) %>%
mutate(diff = abs(ytpred - label))  ## Calculate differences between predicted value and actural value
lmerrTs <- sum(lmtest$diff > 0.5) / len_test ## Calculate test data error ratio
#### LOOCV ####
CV <- c()
for (i in 1:len_train) {
## Using training data to get parameters
fit_LR <- lm(label ~V1 + V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9, data = biopsy_train[-i, ])
## Predict label with tunning data
ytunpred <- predict(fit_LR, newdata = biopsy_train[i, ])
## Calculate differences between predicted value and actural value
gap <- abs(ytunpred - biopsy_train[i, ]$label)
## Get the one LOOCV value
CV[i] <-  sum(gap > 0.5)
}
LOOCV_LR <- mean(CV)
########################################### KNN ##############################################
train_rv <- biopsy_train[, 1:9] ## Get predictors in dataset
test_rv <- biopsy_test[, 1:9]
ks <- seq(1, 228, by = 3) ## Initial K value
TRerr <- TSerr <- CVerr <- rep(0, length(ks)) ## Initial error ratio
for(i in 1:length(ks)){
## training error ratio
tr_res <- knn(train_rv, train_rv, k=ks[i], cl = biopsy_train[,11])
TRerr[i] = sum(tr_res != biopsy_train$label)/ len_train
## test error ratio
ts_res <- knn(train_rv, test_rv, k=ks[i], cl = biopsy_train[,11])
TSerr[i] = sum(ts_res != biopsy_test$label)/ len_test
## LOOCV
cv_res <- knn.cv(train_rv, k=ks[i], cl = biopsy_train[,11])
CVerr[i] = sum(cv_res != biopsy_train$label)/ len_train
}
## summarizing the results in a plot
plot(ks, TRerr, xlab="k", type="l", main="Performance of kNN",
ylab="Error rate", ylim=c(0,0.1), col="black")
lines(ks, TSerr, type="l", col="red")
lines(ks, CVerr, type="l", col="blue")
legend("topleft", legend = c("TRerr", "TSerr", "CVerr"), col=c("black", "red", "blue"),
border = "black",  lty = c(1, 1, 1))
##################################### Logistic Regression ####################################
fit_lgt <- glm(label ~V1 + V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9, data = biopsy_train, family = binomial)
summary(fit_lgt)
#### Training Error ####
p <- predict(fit_lgt, type="response")
logisttrain <-
cbind(biopsy_train, p) %>%
select(label, p) %>%
mutate(diff = abs(p - label))  ## Calculate differences between predicted value and actural value
logisterrTr <- sum(logisttrain$diff > 0.5) / len_train ## Calculate training data error ratio
#### Testing Error ####
## Use coeffient to calculate y -- label
expsum <- fit_lgt$coefficients[1]
for (k in 1:9) {
expsum <- expsum + fit_lgt$coefficients[k+1] * biopsy_test[,k]
}
pTs <- 1/(1 + exp(-expsum))
logisttest <-
cbind(biopsy_test, pTs) %>%
select(label, pTs) %>%
mutate(diff = abs(pTs - label))  ## Calculate differences between predicted value and actural value
logisterrTs <- sum(logisttest$diff > 0.5) / len_test ## Calculate test data error ratio
#### LOOCV ####
CV <- c()
for (i in 1:len_train) {
## Using training data to get parameters
fit_lgt <- glm(label ~V1 + V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9, data = biopsy_train[-i, ], family = binomial)
## Predict label with tunning data
tunning <- biopsy_train[i, ]
expsum <- fit_lgt$coefficients[1]
for (k in 1:9) {
expsum <- expsum + fit_lgt$coefficients[k+1] * tunning[,k]
}
pTu <- 1/(1 + exp(-expsum))
## Calculate differences between predicted value and actural value
gap <- abs(pTu - tunning$label)
## Get the one LOOCV value
CV[i] <-  sum(gap > 0.5)
}
LOOCV_logist <- mean(CV)
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE)
# Chunk 2
# loading packages
library(tidyverse)
library("glmnet")
library("pls")
# Chunk 3: data processing
# loading data
data_spam <- read.table("spambase.data", sep=",", header=FALSE)
# Split data into a training set and a test set
set.seed(1234)
len_data <- nrow(data_spam)
train_num <- sample(len_data, 3065)
spam_train <- data_spam[train_num,]
spam_test <- data_spam[-train_num,]
# Centralize and standardize variables
x_train <- spam_train[,1:57] %>% as.matrix()
y_train <- spam_train[,58]
x_test <- spam_test[,1:57] %>% as.matrix()
y_test <- spam_test[,58]
# Calculate length of test data set for future test error calculating
len_test <- nrow(spam_test)
# Chunk 4
# Fit a LM using Ridge
ridge_fit <- glmnet(x_train, y_train, alpha = 0, family="gaussian" )
# Uisng 10 fold CV to select the constraint value
set.seed(100)
cv_ridge_fit <- cv.glmnet(x_train, y_train, nfold=10, alpha = 0, family="gaussian")
# Chunk 5
# Visulize result of 10-CV
plot(cv_ridge_fit)
# Chunk 6
# Using 1-SE rule to choose lambda for ridge
lambda_ridge <- cv_ridge_fit$lambda.1se
# Chunk 7
# Evaluate performance using test data
y_rg <- predict(cv_ridge_fit, s = lambda_ridge, scale = TRUE, newx = x_test)
# Calculate the test error
rg_terr <- round(mean(as.numeric(y_rg > 0.5) != y_test), 5)
# Chunk 8
# Fit a LM using LASSO
lasso_fit <- glmnet(x_train, y_train, family="gaussian" )
# Uisng 10 fold CV to select the constraint value
set.seed(100)
cv_lasso_fit <- cv.glmnet(x_train, y_train, nfold=10, family="gaussian")
# Chunk 9
# Visulize result of 10-CV
plot(cv_lasso_fit)
# Chunk 10
# Using 1-SE rule to choose lambda for lasso
lambda_lasso <- cv_lasso_fit$lambda.1se
# Chunk 11
# Evaluate performance using test data
y_lp <- predict(cv_lasso_fit, s = lambda_lasso, scale = TRUE, newx = x_test)
# Calculate the test error
lasso_terr <- round(mean(as.numeric(y_lp > 0.5) != y_test), 5)
# Chunk 12
# Uisng 10 fold CV to select the constraint value
set.seed(100)
# Using CV for different alpha
for (i in 1:9) {
assign(paste("cv_eln_fit", i, sep=""),
cv.glmnet(x_train, y_train, nfold=10, alpha = i/10, family="gaussian"))
}
# Create a tibble to show average of mean CV error with different alphas
df_alpha <-
tibble(
Alpha = c("0.1", "0.2", "0.3", "0.4", "0.5", "0.6", "0.7", "0.8", "0.9")
, Mean_CV_Error = c(round(mean(cv_eln_fit1$cvm), 5), round(mean(cv_eln_fit2$cvm), 5),
round(mean(cv_eln_fit3$cvm), 5), round(mean(cv_eln_fit4$cvm), 5),
round(mean(cv_eln_fit5$cvm), 5), round(mean(cv_eln_fit6$cvm), 5),
round(mean(cv_eln_fit7$cvm), 5), round(mean(cv_eln_fit8$cvm), 5),
round(mean(cv_eln_fit9$cvm), 5)))
# Sort value according to error
df_alpha %>% arrange(Mean_CV_Error)
# Chunk 13
cv_eln_fit <- cv.glmnet(x_train, y_train, nfold=10, alpha = 0.7, family="gaussian")
# Chunk 14
# Visulize result of 10-CV
plot(cv_eln_fit)
# Chunk 15
# Using 1-SE rule to choose lambda for Elastic net
lambda_eln <- cv_eln_fit$lambda.1se
# Chunk 16
# Evaluate performance using test data
y_eln <- predict(cv_eln_fit, s = lambda_eln, scale = TRUE, newx = x_test)
# Calculate the test error
eln_terr <- round(mean(as.numeric(y_eln > 0.5) != y_test), 5)
# Chunk 17
# Fit a LM using PCR
set.seed(100)
pcr_fit <- pcr(y_train ~ x_train, data = spam_train, scale = TRUE, ncomp = 56, validation="CV" )
# Get CV error
pcr_fit$validation$PRESS
# Chunk 18
# Draw the etimated coef's
coefplot(pcr_fit, ncomp = 52)
# Chunk 19
# Evaluate performance using test data
y_pcr <- predict(pcr_fit, x_test, scale = TRUE, ncomp = 52)
# Calculate the test error
lasso_terr <- round(mean(as.numeric(y_pcr > 0.5) != y_test), 5)
# Chunk 20
# Fit a LM using PLS
set.seed(100)
pls_fit <- plsr(y_train ~ x_train, data = spam_train, scale = TRUE, ncomp = 56, validation="CV" )
# Get CV error
pls_fit$validation$PRESS
# Chunk 21
# Draw the etimated coef's
coefplot(pls_fit, ncomp = 3)
# Chunk 22
# Evaluate performance using test data
y_pls <- predict(pls_fit, x_test, scale = TRUE, ncomp = 3)
# Calculate the test error
lasso_terr <- round(mean(as.numeric(y_pls > 0.5) != y_test), 5)
# Chunk 23
# create a tibble to show the result
df_performance <-
tibble(
Statistical_Methods = c("ridge", "LASSO", "Elastic net", "PCR", "PLS")
, Test_Error = c(round(ridge_terr, 5), round(lasso_terr, 5),
round(eln_terr, 5), round(pcr_terr, 5),
round(pls_terr, 5))
)
df_performance
df_performance <-
tibble(
Statistical_Methods = c("ridge", "LASSO", "Elastic net", "PCR", "PLS")
, Test_Error = c(round(rg_terr, 5), round(lasso_terr, 5),
round(eln_terr, 5), round(pcr_terr, 5),
round(pls_terr, 5))
)
df_performance
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE)
# Chunk 2
# loading packages
library(tidyverse)
library("glmnet")
library("pls")
# Chunk 3: data processing
# loading data
data_spam <- read.table("spambase.data", sep=",", header=FALSE)
# Split data into a training set and a test set
set.seed(1234)
len_data <- nrow(data_spam)
train_num <- sample(len_data, 3065)
spam_train <- data_spam[train_num,]
spam_test <- data_spam[-train_num,]
# Centralize and standardize variables
x_train <- spam_train[,1:57] %>% as.matrix()
y_train <- spam_train[,58]
x_test <- spam_test[,1:57] %>% as.matrix()
y_test <- spam_test[,58]
# Calculate length of test data set for future test error calculating
len_test <- nrow(spam_test)
# Chunk 4
# Fit a LM using Ridge
ridge_fit <- glmnet(x_train, y_train, alpha = 0, family="gaussian" )
# Uisng 10 fold CV to select the constraint value
set.seed(100)
cv_ridge_fit <- cv.glmnet(x_train, y_train, nfold=10, alpha = 0, family="gaussian")
# Chunk 5
# Visulize result of 10-CV
plot(cv_ridge_fit)
# Chunk 6
# Using 1-SE rule to choose lambda for ridge
lambda_ridge <- cv_ridge_fit$lambda.1se
# Chunk 7
# Evaluate performance using test data
y_rg <- predict(cv_ridge_fit, s = lambda_ridge, scale = TRUE, newx = x_test)
# Calculate the test error
rg_terr <- round(mean(as.numeric(y_rg > 0.5) != y_test), 5)
# Chunk 8
# Fit a LM using LASSO
lasso_fit <- glmnet(x_train, y_train, family="gaussian" )
# Uisng 10 fold CV to select the constraint value
set.seed(100)
cv_lasso_fit <- cv.glmnet(x_train, y_train, nfold=10, family="gaussian")
# Chunk 9
# Visulize result of 10-CV
plot(cv_lasso_fit)
# Chunk 10
# Using 1-SE rule to choose lambda for lasso
lambda_lasso <- cv_lasso_fit$lambda.1se
# Chunk 11
# Evaluate performance using test data
y_lp <- predict(cv_lasso_fit, s = lambda_lasso, scale = TRUE, newx = x_test)
# Calculate the test error
lasso_terr <- round(mean(as.numeric(y_lp > 0.5) != y_test), 5)
# Chunk 12
# Uisng 10 fold CV to select the constraint value
set.seed(100)
# Using CV for different alpha
for (i in 1:9) {
assign(paste("cv_eln_fit", i, sep=""),
cv.glmnet(x_train, y_train, nfold=10, alpha = i/10, family="gaussian"))
}
# Create a tibble to show average of mean CV error with different alphas
df_alpha <-
tibble(
Alpha = c("0.1", "0.2", "0.3", "0.4", "0.5", "0.6", "0.7", "0.8", "0.9")
, Mean_CV_Error = c(round(mean(cv_eln_fit1$cvm), 5), round(mean(cv_eln_fit2$cvm), 5),
round(mean(cv_eln_fit3$cvm), 5), round(mean(cv_eln_fit4$cvm), 5),
round(mean(cv_eln_fit5$cvm), 5), round(mean(cv_eln_fit6$cvm), 5),
round(mean(cv_eln_fit7$cvm), 5), round(mean(cv_eln_fit8$cvm), 5),
round(mean(cv_eln_fit9$cvm), 5)))
# Sort value according to error
df_alpha %>% arrange(Mean_CV_Error)
# Chunk 13
cv_eln_fit <- cv.glmnet(x_train, y_train, nfold=10, alpha = 0.7, family="gaussian")
# Chunk 14
# Visulize result of 10-CV
plot(cv_eln_fit)
# Chunk 15
# Using 1-SE rule to choose lambda for Elastic net
lambda_eln <- cv_eln_fit$lambda.1se
# Chunk 16
# Evaluate performance using test data
y_eln <- predict(cv_eln_fit, s = lambda_eln, scale = TRUE, newx = x_test)
# Calculate the test error
eln_terr <- round(mean(as.numeric(y_eln > 0.5) != y_test), 5)
# Chunk 17
# Fit a LM using PCR
set.seed(100)
pcr_fit <- pcr(y_train ~ x_train, data = spam_train, scale = TRUE, ncomp = 56, validation="CV" )
# Get CV error
pcr_fit$validation$PRESS
# Chunk 18
# Draw the etimated coef's
coefplot(pcr_fit, ncomp = 52)
# Chunk 19
# Evaluate performance using test data
y_pcr <- predict(pcr_fit, x_test, scale = TRUE, ncomp = 52)
# Calculate the test error
pcr_terr <- round(mean(as.numeric(y_pcr > 0.5) != y_test), 5)
# Chunk 20
# Fit a LM using PLS
set.seed(100)
pls_fit <- plsr(y_train ~ x_train, data = spam_train, scale = TRUE, ncomp = 56, validation="CV" )
# Get CV error
pls_fit$validation$PRESS
# Chunk 21
# Draw the etimated coef's
coefplot(pls_fit, ncomp = 3)
# Chunk 22
# Evaluate performance using test data
y_pls <- predict(pls_fit, x_test, scale = TRUE, ncomp = 3)
# Calculate the test error
pls_terr <- round(mean(as.numeric(y_pls > 0.5) != y_test), 5)
# Chunk 23
# create a tibble to show the result
df_performance <-
tibble(
Statistical_Methods = c("ridge", "LASSO", "Elastic net", "PCR", "PLS")
, Test_Error = c(round(rg_terr, 5), round(lasso_terr, 5),
round(eln_terr, 5), round(pcr_terr, 5),
round(pls_terr, 5))
)
df_performance
