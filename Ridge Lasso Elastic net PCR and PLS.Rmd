---
title: "PUBH7475 Homework II"
author: "Renchang Lu 5504577"
date: Feb 10, 2019
output: 
  html_document:
    theme: cerulean
    highlight: tango
    df_print: kable
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

&nbsp;

#### **Introduction**

<font size=3>
In this paper, I will apply **ridge, LASSO, Elastic net, PCR, PLS** to Spam data. To be specific, I will take a random subset with 3065 observations as a training set, and the remaining ones as a test set. Then I will use the test set to evaluate these classifiers.
In Spam data, there are p = 57 variables to distinguish two classes, spam (coded as 1) and email (coded as 0). There are total 1813 spams and 2788 emails.
</font> 

&nbsp;

#### **Data Processing**

<font size=3>
In this part, I load "tidyverse" package at first, which is good at data processing. Also. load "glmnet" and "pls" package, which are for statistical methods. Then, I load the data from "spambase.data" file and split data into training set with **3065** observations and test set with 1536 observations. I don't standardize the data here because some functions has methods to standardize data.
</font> 

```{r, load_packages, include = FALSE}
# loading packages
library(tidyverse) 
library("glmnet")
library("pls")
```

```{r data processing}
# loading data
data_spam <- read.table("spambase.data", sep=",", header=FALSE)

# Split data into a training set and a test set
set.seed(1234) 
len_data <- nrow(data_spam) 
train_num <- sample(len_data, 3065) 
spam_train <- data_spam[train_num,] 
spam_test <- data_spam[-train_num,]

# Centralize and standardize variables
x_train <- spam_train[,1:57] %>% as.matrix()
y_train <- spam_train[,58]
x_test <- spam_test[,1:57] %>% as.matrix()
y_test <- spam_test[,58]

# Calculate length of test data set for future test error calculating
len_test <- nrow(spam_test)

```

&nbsp;

#### **Five Classification Methods**

<font size=3>
As mentioned above, I will apply five classification methods:

1. **Ridge** 
```{r}
# Fit a LM using Ridge
ridge_fit <- glmnet(x_train, y_train, alpha = 0, family="gaussian" )
# Uisng 10 fold CV to select the constraint value
set.seed(100)
cv_ridge_fit <- cv.glmnet(x_train, y_train, nfold=10, alpha = 0, family="gaussian")

```

```{r, fig.align='center'}
# Visulize result of 10-CV
plot(cv_ridge_fit)

```


```{r}
# Using 1-SE rule to choose lambda for ridge
lambda_ridge <- cv_ridge_fit$lambda.1se
```
Therefore, I choose `r lambda_ridge` as lambda for ridge.
</font> 

```{r}
# Evaluate performance using test data
y_rg <- predict(cv_ridge_fit, s = lambda_ridge, scale = TRUE, newx = x_test)
# Calculate the test error
rg_terr <- round(mean(as.numeric(y_rg > 0.5) != y_test), 5)

```

&nbsp;

<font size=3>
2. **Lasso** 

```{r}
# Fit a LM using LASSO
lasso_fit <- glmnet(x_train, y_train, family="gaussian" )
# Uisng 10 fold CV to select the constraint value
set.seed(100)
cv_lasso_fit <- cv.glmnet(x_train, y_train, nfold=10, family="gaussian")

```

```{r, fig.align='center'}
# Visulize result of 10-CV
plot(cv_lasso_fit)

```


```{r}
# Using 1-SE rule to choose lambda for lasso
lambda_lasso <- cv_lasso_fit$lambda.1se
```
Therefore, I choose `r lambda_lasso` as lambda for lasso.
</font> 

```{r}
# Evaluate performance using test data
y_lp <- predict(cv_lasso_fit, s = lambda_lasso, scale = TRUE, newx = x_test)
# Calculate the test error
lasso_terr <- round(mean(as.numeric(y_lp > 0.5) != y_test), 5)

```

&nbsp;

<font size=3>
3. **Elastic net**

In Elastic net method, I choose 0.1, 0.2, 0.3, ... , 0.9 as alpha to find the minimum 10 fold CV error. Then set alpha with minimum 10 fold CV error to the model.

```{r}
# Uisng 10 fold CV to select the constraint value
set.seed(100)

# Using CV for different alpha
for (i in 1:9) {
    assign(paste("cv_eln_fit", i, sep=""), 
           cv.glmnet(x_train, y_train, nfold=10, alpha = i/10, family="gaussian"))
}

# Create a tibble to show average of mean CV error with different alphas
df_alpha <-
    tibble(
        Alpha = c("0.1", "0.2", "0.3", "0.4", "0.5", "0.6", "0.7", "0.8", "0.9")
        , Mean_CV_Error = c(round(mean(cv_eln_fit1$cvm), 5), round(mean(cv_eln_fit2$cvm), 5),
                        round(mean(cv_eln_fit3$cvm), 5), round(mean(cv_eln_fit4$cvm), 5),
                        round(mean(cv_eln_fit5$cvm), 5), round(mean(cv_eln_fit6$cvm), 5),
                        round(mean(cv_eln_fit7$cvm), 5), round(mean(cv_eln_fit8$cvm), 5),
                        round(mean(cv_eln_fit9$cvm), 5)))
# Sort value according to error
df_alpha %>% arrange(Mean_CV_Error)

```
According to the table above, I choose alpha = 0.7 to fit the model.

```{r}
cv_eln_fit <- cv.glmnet(x_train, y_train, nfold=10, alpha = 0.7, family="gaussian")

```


```{r, fig.align='center'}
# Visulize result of 10-CV
plot(cv_eln_fit)
```


```{r}
# Using 1-SE rule to choose lambda for Elastic net
lambda_eln <- cv_eln_fit$lambda.1se
```
Therefore, I choose `r lambda_eln` as lambda for Elastic net.
</font> 

```{r}
# Evaluate performance using test data
y_eln <- predict(cv_eln_fit, s = lambda_eln, scale = TRUE, newx = x_test)
# Calculate the test error
eln_terr <- round(mean(as.numeric(y_eln > 0.5) != y_test), 5)

```

&nbsp;

<font size=3>
4. **PCR**

```{r}
# Fit a LM using PCR
set.seed(100)
pcr_fit <- pcr(y_train ~ x_train, data = spam_train, scale = TRUE, ncomp = 56, validation="CV" )
# Get CV error
pcr_fit$validation$PRESS
```

According to the CV errors, when using 52 components, it has minimum CV error:`r min(pcr_fit$validation$PRESS)`. Therefore, I choose 45 components to fit the model and draw the etimated coef's.
</font> 

```{r, fig.align='center'}
# Draw the etimated coef's
coefplot(pcr_fit, ncomp = 52)
```

```{r}
# Evaluate performance using test data
y_pcr <- predict(pcr_fit, x_test, scale = TRUE, ncomp = 52)
# Calculate the test error
pcr_terr <- round(mean(as.numeric(y_pcr > 0.5) != y_test), 5)
```

&nbsp;

<font size=3>
5. **PLS**

```{r}
# Fit a LM using PLS
set.seed(100)
pls_fit <- plsr(y_train ~ x_train, data = spam_train, scale = TRUE, ncomp = 56, validation="CV" )
# Get CV error
pls_fit$validation$PRESS
```

According to the CV errors, when using 3 components, it has minimum CV error:`r min(pls_fit$validation$PRESS)`. Therefore, I choose 3 components to fit the model and draw the etimated coef's.

```{r, fig.align='center'}
# Draw the etimated coef's
coefplot(pls_fit, ncomp = 3)
```

```{r}
# Evaluate performance using test data
y_pls <- predict(pls_fit, x_test, scale = TRUE, ncomp = 3)
# Calculate the test error
pls_terr <- round(mean(as.numeric(y_pls > 0.5) != y_test), 5)
```




```{r}
# create a tibble to show the result
df_performance <-
    tibble(
        Statistical_Methods = c("ridge", "LASSO", "Elastic net", "PCR", "PLS")
        , Test_Error = c(round(rg_terr, 5), round(lasso_terr, 5),
                        round(eln_terr, 5), round(pcr_terr, 5),
                        round(pls_terr, 5))
    )
df_performance
```

Obviously, in this case, **PLS** has the minimum test error over all statistical methdos, which is `r round(pls_terr, 5)`
</font> 








